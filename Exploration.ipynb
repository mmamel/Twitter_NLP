{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exploration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4Wsb7FkOn7YckhJPHGTcX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmamel/Twitter_NLP/blob/master/Exploration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48CplMvjaDyy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "eacae64c-32ba-4403-a058-e2265d29d85c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import string\n",
        "import time\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#Implementation is based on Nick Koprowicz word count soluion citation: Koprowicz, N (2020) A simple solution using only word counts (Version 17) [Source code Python Notebook] https://www.kaggle.com/nkoprowicz/a-simple-solution-using-only-word-counts. \n",
        "#Added code was modifying training data to have non selected text and using spacy to word dependency\n",
        "\n",
        "##Note this code was not the best scoring and instead is to reflect the exploration part of the assignment.##\n",
        "##different approachs diverge with the function calculate_selected_text (nonselected text implementation) vs calculate_selected_text_2 (spacy implementation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCUdE4ERdW2B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def replace_string(str1, str2):\n",
        "    temp1 = str1\n",
        "    temp2 = str2\n",
        "    temp = temp1.replace(temp2, \"\")\n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hd9ACnIaJUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load data\n",
        "nlp = spacy.load('en')\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "train.dropna(inplace=True)\n",
        "train['text'] = train['text'].apply(lambda x: x.lower())\n",
        "test['text'] = test['text'].apply(lambda x: x.lower())\n",
        "\n",
        "#Split data for training and validation\n",
        "X_train, X_val = train_test_split(\n",
        "    train, train_size = 0.80, random_state = 0)\n",
        "\n",
        "#prevent setting with copy warning by explicitly stating X_train is independent\n",
        "X_train = X_train.copy()\n",
        "X_val = X_val.copy()\n",
        "\n",
        "#Split training basd on sentiment\n",
        "pos_train = X_train[X_train['sentiment'] == 'positive']\n",
        "neutral_train = X_train[X_train['sentiment'] == 'neutral']\n",
        "neg_train = X_train[X_train['sentiment'] == 'negative']\n",
        "X_train['non_selected'] = X_train.apply(lambda x: replace_string(x['text'],x['selected_text']), axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZmfHkfJbUuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LemmaTokenizer:\n",
        "  def __init__(self):\n",
        "    self.wnl = WordNetLemmatizer()\n",
        "  def __call__(self, doc):\n",
        "    return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
        "\n",
        "\n",
        "tfidf = CountVectorizer(max_df=0.95, min_df=2,max_features =10000, stop_words='english')\n",
        "\n",
        "X_train_cv = tfidf.fit_transform(X_train['text'])\n",
        "\n",
        "X_pos = tfidf.transform(pos_train['text'])\n",
        "X_neutral = tfidf.transform(neutral_train['text'])\n",
        "X_neg = tfidf.transform(neg_train['text'])\n",
        "\n",
        "pos_count_df = pd.DataFrame(X_pos.toarray(), columns=tfidf.get_feature_names())\n",
        "neutral_count_df = pd.DataFrame(X_neutral.toarray(), columns=tfidf.get_feature_names())\n",
        "neg_count_df = pd.DataFrame(X_neg.toarray(), columns=tfidf.get_feature_names())\n",
        "\n",
        "\n",
        "\n",
        "# Create dictionaries of the words within each sentiment group, where the values are the proportions of tweets that \n",
        "# contain those words\n",
        "\n",
        "pos_words = {}\n",
        "neutral_words = {}\n",
        "neg_words = {}\n",
        "non_words = {}\n",
        "\n",
        "for k in tfidf.get_feature_names():\n",
        "    pos = pos_count_df[k].sum()\n",
        "    neutral = neutral_count_df[k].sum()\n",
        "    neg = neg_count_df[k].sum()\n",
        "    pos_words[k] = pos/pos_train.shape[0]\n",
        "    neutral_words[k] = neutral/neutral_train.shape[0]\n",
        "    neg_words[k] = neg/neg_train.shape[0]\n",
        "    \n",
        "X_train_cv_2 = tfidf.fit_transform(X_train['non_selected'])\n",
        "X_non = tfidf.transform(X_train['non_selected'])\n",
        "\n",
        "non_count_df = pd.DataFrame(X_non.toarray(), columns = tfidf.get_feature_names())\n",
        "\n",
        "for k in tfidf.get_feature_names():\n",
        "    non = non_count_df[k].sum()\n",
        "    non_words[k] = non/X_train['non_selected'].shape[0]\n",
        "\n",
        "neg_words_adj = {}\n",
        "pos_words_adj = {}\n",
        "neutral_words_adj = {}\n",
        "non_word_adj = {}\n",
        "\n",
        "#take out repeats\n",
        "for key, value in non_words.items():\n",
        "    non_word_adj[key] = non_words[key]\n",
        "\n",
        "for key, value in neg_words.items():\n",
        "    neg_words_adj[key] = neg_words[key] - (neutral_words[key] + pos_words[key])\n",
        "    \n",
        "for key, value in pos_words.items():\n",
        "    pos_words_adj[key] = pos_words[key] - (neutral_words[key] + neg_words[key])\n",
        "    \n",
        "for key, value in neutral_words.items():\n",
        "    neutral_words_adj[key] = neutral_words[key] - (neg_words[key] + pos_words[key])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8fMznLGbW3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#APROACH TWO - Simple word count to create vocabulary which is fitted on to test cases to generate a probability\n",
        "def calculate_selected_text(df_row, tol = 0):\n",
        "    \n",
        "    tweet = df_row['text']\n",
        "    sentiment = df_row['sentiment']\n",
        "    \n",
        "    if(sentiment == 'neutral'):\n",
        "        return tweet\n",
        "    \n",
        "    elif(sentiment == 'positive'):\n",
        "        dict_to_use = pos_words_adj # Calculate word weights using the pos_words dictionary\n",
        "    elif(sentiment == 'negative'):\n",
        "        dict_to_use = neg_words_adj # Calculate word weights using the neg_words dictionary\n",
        "        \n",
        "    words = tweet.split()\n",
        "    words_len = len(words)\n",
        "\n",
        "    subsets = [words[i:j+1] for i in range(words_len) for j in range(i,words_len)]\n",
        "    score = 0\n",
        "    selection_str = '' # This will be our choice\n",
        "    lst = sorted(subsets, key = len) # Sort candidates by length\n",
        "    doc = nlp(str(words))\n",
        "    for i in range(len(subsets)):\n",
        "        new_sum = 0 # Sum for the current substring\n",
        "        # Calculate the sum of weights for each word in the substring\n",
        "        for p in range(len(lst[i])):\n",
        "            if(lst[i][p].translate(str.maketrans('','',string.punctuation)) in dict_to_use.keys()):\n",
        "                new_sum += dict_to_use[lst[i][p].translate(str.maketrans('','',string.punctuation))]\n",
        "            if(lst[i][p].translate(str.maketrans('','',string.punctuation)) in non_words.keys()):\n",
        "                new_sum -= non_word_adj[lst[i][p].translate(str.maketrans('','',string.punctuation))]\n",
        "        if(new_sum > score + tol):\n",
        "            score = new_sum\n",
        "            selection_str = lst[i]\n",
        "            # tol = tol*5 # Increase the tolerance a bit each time we choose a selection\n",
        "\n",
        "    # If we didn't find good substrings, return the whole text\n",
        "    if(len(selection_str) == 0):\n",
        "        selection_str = words\n",
        "        \n",
        "    return ' '.join(selection_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pvzKMfcjiCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#APROACH 3 - Spacy implementation to analyze sentence structuree\n",
        "\n",
        "def calculate_selected_text_2(df_row, tol = 0):\n",
        "      \n",
        "    tweet = df_row['text']\n",
        "    sentiment = df_row['sentiment']\n",
        "    \n",
        "    if(sentiment == 'neutral'):\n",
        "        return tweet\n",
        "    \n",
        "    elif(sentiment == 'positive'):\n",
        "        dict_to_use = pos_words_adj # Calculate word weights using the pos_words dictionary\n",
        "    elif(sentiment == 'negative'):\n",
        "        dict_to_use = neg_words_adj # Calculate word weights using the neg_words dictionary\n",
        "    text = tweet\n",
        "    text= re.sub(r\"[.]+\", \" \", text)\n",
        "\n",
        "    words = text.split()\n",
        "    words_len = len(words)\n",
        "\n",
        "    max_word = ''\n",
        "    max_val = 0\n",
        "    for i in words:\n",
        "        curr=0\n",
        "        if i.translate(str.maketrans(\"\",\"\", string.punctuation)) in dict_to_use.keys() :\n",
        "            if dict_to_use[i.translate(str.maketrans('','',string.punctuation))] > max_val:\n",
        "                max_val = dict_to_use[i.translate(str.maketrans('','',string.punctuation))]\n",
        "                max_word = i \n",
        "    selection_str = [max_word]\n",
        "    doc = nlp(str(tweet))\n",
        "    for token in doc:\n",
        "        if token.text == max_word:\n",
        "            for x in [child for child in token.children]:\n",
        "                if(str(x) != ' '):\n",
        "                  selection_str.append(str(x))\n",
        "\n",
        "    return ' '.join(selection_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEsDS_NeiJfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.options.mode.chained_assignment = None\n",
        "tol = 0.001\n",
        "\n",
        "X_val['predicted_selection'] = ''\n",
        "\n",
        "for index, row in X_val.iterrows():\n",
        "    \n",
        "    selected_text = calculate_selected_text(row, tol)\n",
        "    \n",
        "    X_val.loc[X_val['textID'] == row['textID'], ['predicted_selection']] = selected_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG4eQTbciMOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluation function \n",
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSdtNhFKiPYz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4988c599-e424-4f8f-ea51-0f5eb736baf9"
      },
      "source": [
        "#Run jaccard function on all test cases to find jaccard score\n",
        "X_val['jaccard'] = X_val.apply(lambda x: jaccard(x['selected_text'], x['predicted_selection']), axis = 1)\n",
        "\n",
        "print('The jaccard score for the validation set is:', np.mean(X_val['jaccard']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The jaccard score for the validation set is: 0.6604283711848614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5tHpj_cyTZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}